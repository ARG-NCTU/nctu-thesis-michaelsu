\chapter{文獻回顧}
\label{chapter:relate-work}

\section{亞馬遜拾取/機器人挑戰(Amazon Picking/Robotics Challanges 2015-2017)}
亞馬遜拾取挑戰 (Amazon Picking Challenge)、亞馬遜機器人挑戰 (Amazon
Robotics Challenge) 是由亞馬遜公司所創辦的大型競賽,主要目的為在
大型倉儲中商品可能高達上千種,層層排列的大型貨架,如以人力來依照訂單
集貨,不僅需要查看商品存放的位置還必須爬上貨梯進行搬運,倘若使用機器
人自動化地拾取商品,能夠大量縮短 集貨的時間。由於真實的倉儲的環境使得
機器人能活動的空間有限,且必須使機器人能夠自動辨認大量的商品種類,抓
取並放置正確的位置,對於倉儲自動化有相當大的推進。在上架任務中,物體
姿態估計是十分重要的一環,以六個自由度之物體姿態主要利用預先建制的三
維 CAD 模型,與深度攝影機所獲得的點雲(Point Cloud)進行點雲匹配如 ICP
(Iterative Closest Point) ~\cite{pomerleau2013comparing},
需以點雲資訊匹配三維模型,將實際物體的點雲從
深度攝影機所得到的原始點雲加以分離,才能獲得較好的效果,利用物體偵測
方法為將二維候選框資訊投影至三維得到物體的三維點雲,有相當大的可能引
入背景點雲進而影響 ICP 匹配結果,而採用物體語意分割(Semantic
Segmentation)的方法能夠將物體輪廓較精確的標記,將二維像素分類結果投影
至三維後能得到較單純的物體點雲,Amazon Picking Challenge 2016 競賽中
MIT-Princeton 團隊於提出的視覺系統~\cite{zeng2016multi}採用了物體語意分割與 ICP 點雲匹配的
方法。

\section{主動式視覺與主動式操作}
~\cite{sunderhauf2018limits}提及機器人視覺與電腦視覺領域的主要差異為:機器人視覺最後產出的為行動
(action)而非電腦視覺的資訊(information),電腦視覺或機器學習常以既有資
料集進行訓練,並著重於資料集的準確度提升,但機器人視覺則與環境互動,特
別關注:主動式機器人視覺(active vision)、主動式操作(active manipulation)。主
動式機器人視覺可透過機器人去控制移動攝影機,改變其位置與觀察視角,藉此去得到更多的環境資訊去改善優化機器人對環境的感知能力,改善系統的感
知信心指數、解決感知上的歧義、最小化被遮蔽的、反射等問題。主動式操作則是進一步利用會去改變場景去幫助感知的效果,例如:機器手臂可以去移動
被遮擋的物品去得到被隱藏的資訊。
這樣的概念被應用於這些研究中： \cite{atanasov2014nonmyopic} \cite{doumanoglou2016recovering} \cite{malmir2017deep}採取主動式機器人視覺的概念，採用``next-best viewpoint''的策略去改善感知的信心指數，並應用於物件偵測上。
 \cite{zeng2018robotic} 則偏向更高階的主動式操作，採取``grasp-first-then-recognize''策略去改善雜亂環境中糟糕的感知效果。
本研究就是藉由主動式操作,對於商標文字被遮蔽的物件進行操作,進而找出商標物件並正確進行上架。



\section{夾取成功率預測}
物品夾取成功率(Object affordance )對於機器人操作領域是一個重要的議題，而且其演算法往往與末端效應器（end-effector）有高度的相關。
為了處理雜亂、互相遮蔽的環境、與不同幾何形狀的物品，有許多最新的研究採取同時結合多種演算法並搭配客製化的末端效應器：
~\cite{zeng2016multi}透過經典的姿態預測：　物件模型比對（object　registration）去決定操作夾取物件的方法
~\cite{zeng2018robotic}定義4個操作方法去夾取或吸取物品，並訓練兩個全卷積網路去進行像素級別的物品夾取成功率預測
此外，也有許多研究專門研究使用兩指假爪夾取物品的物品夾取成功率： \cite{redmon2015real} 在圖片中只會有一個最佳的夾取姿態的假設，將一張彩色圖片編碼成許多網格（grid cell），並預測最終的夾取位置與姿態。另一方面，在只觀察物體的深度下，~\cite{mahler2017dex}先建立多個候選夾取姿態，定將這些姿態投影到圖片上，並預測這些夾取姿態的夾取成功率以找到最適合的夾取方法。
~\cite{pinto2016supersizing} 採取了多段的學習方法，其融合了卷積網路神經系統（CNN）與強化式學習 （reinforcement learning）去學習. \cite{zeng2018learning}透過將兩個平行的全卷積網路（一個專門訓練推，另一個專門旋練夾取）與Q-learning架構，決定現在該進行夾取或是該將原本緊密的物件們先分開再夾取的策略。以上提出的方法皆有效的改善了雜亂環境與物體互相遮蔽的問題，但卻未考慮到夾取後能否用特定姿勢去放置物品。

\section{雙機械手臂協作}
雖然機器人操作領域的問題已經被大量研究，但都僅限於單隻手臂的情況下, 關於雙手臂操作的應用研究卻較少被關注. 在Smith \textit{et al.} \cite{smith2012dual}研究中,
提出關於雙手臂應用大致可分為兩類：goal-coordinated 方法(再同個環境執行相同任務，但彼此分開行動，無任何合作), 與bimanual manipulation (對同一個物體同時進行操作).
~\cite{schwarz2018fast}便是goal-coordinated approach方法的一個典型例子，其透過兩隻手臂在雜亂盒子裡執行夾取任務，只專注於夾取物品與避免手臂之間互相碰撞，手臂間無合作，
但有效的提升夾取的效率。 而~\cite{harada2012pick} 考慮物品姿態並針對多種形狀物體進行夾取放置任務，系統透過雙手臂執行重夾動作以達到理想的放置姿態。
~\cite{miyazaki2017object}設計一個雙手臂系統，雙手臂各自有不同功能：將物品掃開解決遮蔽問題、使用吸盤假爪從雜亂的盤子中夾取物體。在Smith分類底下，本研究可被歸類為bimanual：　系統目標處理品牌文字被遮蔽的物品，透過雙手臂（一為使用吸盤假爪夾取物品至空中以便觀察，二為用兩指假爪穩定夾取並放置物體到架上）。

\section{機器人操作領域之基準訓練測試資料集}
在電腦視覺領域，已有許多資料集被作為基準資料庫，如Pascal VOC資料集 ~\cite{everingham2010pascal}共有20類物品，並提供Bounding box與像素集別的標註，供大家用來評估訓練模型的準確度。Freiburg Groceries資料集 ~\cite{jund2016freiburg}，則提供5000張常見的日常食用商品，有25個類別共5000張彩色圖片及圖片分類，這些雖都是專門用來評估影像辨識的資料集，並且已受大家公認且大量使用的資料集，但卻與機器人操作無太多關聯。而在機器人操作領域上，目前並無一個一個公認的資料集做為測試用，但仍有不少研究涉及到：MIT Princeton 團隊所建立的``Shelf \& Tote''資料集~\cite{zeng2016multi}是為了Amazon Picking Challenge，所建立的資料集，此資料集針對當中的39類物品，有完整的彩色圖、深度圖、以及像素及別的標註。也有提供競賽時的場景做為測試集。此測試集可有效評估三維姿態辨識的準確度，本研究所建立的資料集，有數個也挑選競賽所用之物品。Yale-CMU-Berkeley資料集~\cite{calli2015benchmarking}則提供77個物品的高解析度彩色圖、深度圖、以及具有高解析度材質(texture)的三維模型。並分為5類(食物、廚房用具類、工具類、形狀類、任務類)，並保證這些物品的來源性，希望能作為機器人操作之基準訓練測試資料集。有許多研究如~\cite{mahler2016dex}便使用其資料與模型。有研究也專門建置評估夾取姿態好壞的資料集：VisGraB資料集~\cite{kootstra2012visgrab}結合了現實與模擬的實驗設定，在現實場景與模擬場景中。使用者可使用這個資料集，在模擬場景中用其所設計的評量標準評估演算法所預測出的夾取姿態可行性高低。本研究參考以上許多研究作法，最後採用多視角的自動資料蒐集方式，並同時從虛擬與現實蒐集與標註資料。衡量標準則分別採用真實世界測試集場域去評估夾取可行性與商品語意切割準確度。
